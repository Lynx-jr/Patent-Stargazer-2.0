{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEK-_hRXwPcE",
        "outputId": "6a32e281-822c-43e7-9d59-4bb1d4606e8e"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-04 13:36:44--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-06-04 13:36:44--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-04 13:36:44 (25.8 MB/s) - written to stdout [1608/1608]\n",
            "\n",
            "setup Colab for PySpark 3.0.2 and Spark NLP 3.0.3\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,616 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.6 kB]\n",
            "Fetched 2,902 kB in 3s (894 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE0SSmO0ws4R",
        "outputId": "57817815-ba57-4998-b339-c01cef587ef2"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version: 3.0.3\n",
            "Apache Spark version: 3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wEp_MhiwmCq",
        "outputId": "8dba88be-9d78-441f-bc55-2827e84e2799"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bRqgbNexsja",
        "outputId": "b8f00a13-8d64-4e93-b8bb-74a13484b8da"
      },
      "source": [
        "import wget\n",
        "wget.download('https://s3.amazonaws.com/data.patentsview.org/download/patent.tsv.zip','sample_data/patent.tsv.zip')\n",
        "\n",
        "from zipfile import ZipFile\n",
        "  \n",
        "# specifying the zip file name\n",
        "file_name = \"sample_data/patent.tsv.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    zip.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Name                                             Modified             Size\n",
            "patent.tsv                                     2021-03-04 23:42:36   6005805218\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcYhYDDkxu1o",
        "outputId": "90b51e09-45de-4f16-9361-e9791deecc3d"
      },
      "source": [
        "data = spark.read.csv('patent.tsv',\n",
        "                      sep=\"\\t\",\n",
        "                      header=True,\n",
        "                      inferSchema=True)\n",
        "data.printSchema()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- number: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- kind: string (nullable = true)\n",
            " |-- num_claims: integer (nullable = true)\n",
            " |-- filename: string (nullable = true)\n",
            " |-- withdrawn: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgHEvugYx_wL",
        "outputId": "99bf21f2-28ef-4212-95e9-59d8f9729907"
      },
      "source": [
        "data.show(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+\n",
            "|      id|   type|  number|country|      date|            abstract|               title|kind|num_claims|     filename|withdrawn|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+\n",
            "|10000000|utility|10000000|     US|2018-06-19|A frequency modul...|Coherent LADAR us...|  B2|        20|ipg180619.xml|        0|\n",
            "|10000001|utility|10000001|     US|2018-06-19|The injection mol...|Injection molding...|  B2|        12|ipg180619.xml|        0|\n",
            "|10000002|utility|10000002|     US|2018-06-19|The present inven...|Method for manufa...|  B2|         9|ipg180619.xml|        0|\n",
            "|10000003|utility|10000003|     US|2018-06-19|The invention rel...|Method for produc...|  B2|        18|ipg180619.xml|        0|\n",
            "|10000004|utility|10000004|     US|2018-06-19|The present inven...|Process of obtain...|  B2|         6|ipg180619.xml|        0|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h81EM5NUyuL1"
      },
      "source": [
        "data = data.filter(data['withdrawn']==0)\n",
        "data = data.withColumn('year',data['date'].substr(1,4))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK_GyOSqyws5"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "data = data.withColumn('year',data['year'].cast(IntegerType()))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wK-iLaOy2Q3",
        "outputId": "7fe125cd-e7bc-498e-a298-99077e829a3c"
      },
      "source": [
        "data.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- number: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- abstract: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- kind: string (nullable = true)\n",
            " |-- num_claims: integer (nullable = true)\n",
            " |-- filename: string (nullable = true)\n",
            " |-- withdrawn: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUjeWJROz9B-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9Anxp_4y9e4"
      },
      "source": [
        "# **Prepocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnu2pVPhzPmn"
      },
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('title') \\\n",
        "     .setOutputCol('document')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQow7vtTzPyC"
      },
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx3hkb7UzW9k"
      },
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ygDtcZPzcsG",
        "outputId": "62ff55f1-7725-4918-ecb2-22c42cc9a4af"
      },
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxlpU8dUzfR4",
        "outputId": "86f6d309-eff4-4ac5-a3ce-f312aa785978"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-14fp85gziFT"
      },
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UT4mQMwzkjr"
      },
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JFUXcGpzm0e",
        "outputId": "c47130c2-6fa5-4360-d571-6af6703fc815"
      },
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYve96QszrZg"
      },
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmPJh_fVz2LD"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGTjYQIkz5rF"
      },
      "source": [
        "processed_review = pipeline.fit(data).transform(data)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrMm0psR0Bzc",
        "outputId": "c0cd946b-3c1f-454e-b393-2bc8b3f377d5"
      },
      "source": [
        "processed_review.limit(5).show()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+\n",
            "|      id|   type|  number|country|      date|            abstract|               title|kind|num_claims|     filename|withdrawn|year|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+\n",
            "|10000000|utility|10000000|     US|2018-06-19|A frequency modul...|Coherent LADAR us...|  B2|        20|ipg180619.xml|        0|2018|[coherent, ladar,...|[coherent, ladar,...|[JJ, NN, NN, NN, ...|\n",
            "|10000001|utility|10000001|     US|2018-06-19|The injection mol...|Injection molding...|  B2|        12|ipg180619.xml|        0|2018|[injection, moldi...|[injection, moldi...|[NN, NN, NN, CC, ...|\n",
            "|10000002|utility|10000002|     US|2018-06-19|The present inven...|Method for manufa...|  B2|         9|ipg180619.xml|        0|2018|[method, manufact...|[method, for, man...|[NN, IN, VBG, NN,...|\n",
            "|10000003|utility|10000003|     US|2018-06-19|The invention rel...|Method for produc...|  B2|        18|ipg180619.xml|        0|2018|[method, produce,...|[method, for, pro...|[NN, IN, NN, DT, ...|\n",
            "|10000004|utility|10000004|     US|2018-06-19|The present inven...|Process of obtain...|  B2|         6|ipg180619.xml|        0|2018|[process, obtain,...|[process, of, obt...|[NN, IN, NN, DT, ...|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t9JRyUC0PxQ"
      },
      "source": [
        "from pyspark.sql import types as T\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66En7j8w0We-"
      },
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZV0pf2h0eBF"
      },
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-yxBvug0gQh"
      },
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49AoiCll0iz-"
      },
      "source": [
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) \\"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXr4ieDb0krp"
      },
      "source": [
        "pos_pipeline = Pipeline() \\\n",
        "     .setStages([pos_documentAssembler,                  \n",
        "                 pos_tokenizer,\n",
        "                 pos_ngrammer,  \n",
        "                 pos_finisher])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNNtfOEZ0mmV"
      },
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t95I8vuJ0oWv",
        "outputId": "aa741dc7-e6ee-4933-a10f-5dd6061a1879"
      },
      "source": [
        "processed_review.columns"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'type',\n",
              " 'number',\n",
              " 'country',\n",
              " 'date',\n",
              " 'abstract',\n",
              " 'title',\n",
              " 'kind',\n",
              " 'num_claims',\n",
              " 'filename',\n",
              " 'withdrawn',\n",
              " 'year',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bce8ZVqT0sWC",
        "outputId": "53765734-e4c7-4851-ba96-e7e9ec9f5b83"
      },
      "source": [
        "processed_review.show(5)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "|      id|   type|  number|country|      date|            abstract|               title|kind|num_claims|     filename|withdrawn|year|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "|10000000|utility|10000000|     US|2018-06-19|A frequency modul...|Coherent LADAR us...|  B2|        20|ipg180619.xml|        0|2018|[coherent, ladar,...|[coherent, ladar,...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|\n",
            "|10000001|utility|10000001|     US|2018-06-19|The injection mol...|Injection molding...|  B2|        12|ipg180619.xml|        0|2018|[injection, moldi...|[injection, moldi...|[NN, NN, NN, CC, ...|[NN, NN, NN, CC, ...|\n",
            "|10000002|utility|10000002|     US|2018-06-19|The present inven...|Method for manufa...|  B2|         9|ipg180619.xml|        0|2018|[method, manufact...|[method, for, man...|[NN, IN, VBG, NN,...|[NN, IN, VBG, NN,...|\n",
            "|10000003|utility|10000003|     US|2018-06-19|The invention rel...|Method for produc...|  B2|        18|ipg180619.xml|        0|2018|[method, produce,...|[method, for, pro...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|\n",
            "|10000004|utility|10000004|     US|2018-06-19|The present inven...|Process of obtain...|  B2|         6|ipg180619.xml|        0|2018|[process, obtain,...|[process, of, obt...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARYEJJKQ0tbb"
      },
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01q6CvqD3QB-",
        "outputId": "31edabe5-86ba-4e8d-df68-4fbf433f5f97"
      },
      "source": [
        "processed_review.columns"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['id',\n",
              " 'type',\n",
              " 'number',\n",
              " 'country',\n",
              " 'date',\n",
              " 'abstract',\n",
              " 'title',\n",
              " 'kind',\n",
              " 'num_claims',\n",
              " 'filename',\n",
              " 'withdrawn',\n",
              " 'year',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N74JdBlsDxgs",
        "outputId": "283f1032-f3d0-46ce-e80b-8fdac4894e90"
      },
      "source": [
        "processed_review.show(5)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "|      id|   type|  number|country|      date|            abstract|               title|kind|num_claims|     filename|withdrawn|year|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "|10000000|utility|10000000|     US|2018-06-19|A frequency modul...|Coherent LADAR us...|  B2|        20|ipg180619.xml|        0|2018|[coherent, ladar,...|[coherent, ladar,...|[JJ, NN, NN, NN, ...|[JJ, NN, NN, NN, ...|\n",
            "|10000001|utility|10000001|     US|2018-06-19|The injection mol...|Injection molding...|  B2|        12|ipg180619.xml|        0|2018|[injection, moldi...|[injection, moldi...|[NN, NN, NN, CC, ...|[NN, NN, NN, CC, ...|\n",
            "|10000002|utility|10000002|     US|2018-06-19|The present inven...|Method for manufa...|  B2|         9|ipg180619.xml|        0|2018|[method, manufact...|[method, for, man...|[NN, IN, VBG, NN,...|[NN, IN, VBG, NN,...|\n",
            "|10000003|utility|10000003|     US|2018-06-19|The invention rel...|Method for produc...|  B2|        18|ipg180619.xml|        0|2018|[method, produce,...|[method, for, pro...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|\n",
            "|10000004|utility|10000004|     US|2018-06-19|The present inven...|Process of obtain...|  B2|         6|ipg180619.xml|        0|2018|[process, obtain,...|[process, of, obt...|[NN, IN, NN, DT, ...|[NN, IN, NN, DT, ...|\n",
            "+--------+-------+--------+-------+----------+--------------------+--------------------+----+----------+-------------+---------+----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGGBzVZS2EWI"
      },
      "source": [
        "# **Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lc1mbS72Ivj"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='finished_ngrams', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt5Zr44W3YQB"
      },
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1pY87f73ZhR"
      },
      "source": [
        "# **LDA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCWySk1F3brg"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 6\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNCLHvf_3fzP"
      },
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8RocYc53zHx",
        "outputId": "4ff8a5e3-3dc2-4e97-c7c2-e584180b0ef4"
      },
      "source": [
        "num_top_words = 20\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------------------------------------------------------------------------------------+\n",
            "|topic|                                                                          topicWords|\n",
            "+-----+------------------------------------------------------------------------------------+\n",
            "|    0|                    [with, for, a, and, of, use, control, assembly, method, produce]|\n",
            "|    1|                [a, for, and, method, apparatus, device, system, of, in, method_and]|\n",
            "|    2|                   [of, and, method, for, device, method_of, the, a, apparatus, use]|\n",
            "|    3|                  [and, a, method, for, of, device, display, apparatus, system, the]|\n",
            "|    4|[and, method, for, system, of, a, and_method, method_and, communication, method_for]|\n",
            "|    5| [process, of, for, method, and, method_for, and_method, device, the, semiconductor]|\n",
            "+-----+------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}